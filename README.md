Got it âœ… Letâ€™s craft a **professional and advanced `README.md`** for your GitHub repo.
This will look polished, highlight your skills, and guide users through your project like a real-world data science portfolio.

---

# ğŸ“Š CORD-19 Metadata Analysis & Interactive Explorer

[![Streamlit](https://img.shields.io/badge/Framework-Streamlit-ff4b4b)](https://streamlit.io/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue)](https://www.python.org/)
[![Pandas](https://img.shields.io/badge/Library-pandas-yellow)](https://pandas.pydata.org/)
[![Matplotlib](https://img.shields.io/badge/Library-matplotlib-orange)](https://matplotlib.org/)
[![Seaborn](https://img.shields.io/badge/Library-seaborn-green)](https://seaborn.pydata.org/)

This project explores the **CORD-19 metadata dataset** (COVID-19 research papers) and demonstrates a **complete data science workflow**:

1. Data loading & exploration
2. Data cleaning & preparation
3. Descriptive analysis & visualization
4. Interactive **Streamlit web app** for exploration
5. Documentation & reflection

---

## ğŸ“‚ Project Structure

```
Frameworks_Assignment/
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for step-by-step analysis
â”‚   â”œâ”€â”€ Part1_Data_Exploration.ipynb
â”‚   â”œâ”€â”€ Part2_Data_Cleaning.ipynb
â”‚   â”œâ”€â”€ Part3_Analysis_Visualization.ipynb
â”‚   â””â”€â”€ Part5_Reflection.ipynb
â”‚
â”œâ”€â”€ app.py                      # Streamlit application
â”œâ”€â”€ metadata_clean.csv          # Cleaned dataset (sample from Kaggle CORD-19)
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                   # Project documentation
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/Frameworks_Assignment.git
cd Frameworks_Assignment
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Streamlit App

```bash
streamlit run app.py
```

---

## ğŸ“Š Key Findings

* **Dataset Overview**

  * Final cleaned dataset: ~29,491 records Ã— 10 columns
  * Major missing values in `publish_time` (~96%)
  * All records retain titles, abstracts, authors, and journals after cleaning

* **Temporal Trends**

  * Publications span **2006â€“2020**
  * Massive spike in **2020** with 1,100+ papers

* **Top Journals**

  * *PLoS One*, *Emerg Infect Dis*, *Sci Rep*, *PLoS Pathog* lead publication counts

* **Sources**

  * Data aggregated from multiple repositories (PubMed, PMC, WHO, etc.)

* **Text Insights**

  * Titles average 8â€“15 words
  * Abstracts vary from ~50â€“300 words
  * Frequent terms: *COVID-19*, *coronavirus*, *infection*, *respiratory*

---

## ğŸ“· Sample Visualizations

### Publications by Year

![Publications by Year](docs/publications_by_year.png)

### Word Cloud of Titles

![Word Cloud](docs/wordcloud_titles.png)

### Top Journals

![Top Journals](docs/top_journals.png)

---

## ğŸŒ Streamlit App Features

âœ… Interactive **year range slider**
âœ… **Dropdown filter** by source
âœ… ğŸ“Š Charts: publications trend, top journals, top sources
âœ… â˜ Word cloud of research titles
âœ… ğŸ” Raw data preview

---

## ğŸ” Reflection

### Challenges

* **High missingness** in `publish_time` limited full temporal analysis
* **Authors field** contained unnormalized strings, complicating per-author stats
* Dataset size required sampling for efficient analysis

### Learnings

* Practical experience with **data cleaning strategies** (dropping vs imputing)
* Text analysis basics with **word frequency & word clouds**
* Building interactive dashboards using **Streamlit**
* End-to-end workflow: *load â†’ clean â†’ analyze â†’ visualize â†’ deploy*

---

## ğŸ“œ License

This project is for educational purposes. Data source: [CORD-19 Dataset (Allen Institute for AI)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).

---

## ğŸ™Œ Acknowledgements

* [Allen Institute for AI](https://allenai.org/) for dataset
* [Streamlit](https://streamlit.io/) for rapid app development
* [Kaggle](https://www.kaggle.com/) community for open datasets

---

âœ¨ Developed as part of **Frameworks Assignment** â€” demonstrating data science fundamentals with a real-world dataset.

---
